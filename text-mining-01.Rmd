---
title: 'Text Mining with R'
subtitle: '(part 1)'
output: html_document
---

***

## Chapter 1: The Tidy Text Format

***

**token**: *meaningful unit of text for analysis*

**tokenization**: *breaking text into tokens*

```{r, message = F, echo = F}
# Loading in packages...

library(tidytext)
library(scales)
library(tidyverse)

# Loading in data...


```

```{r, echo = T, eval = F}
# Emily Dickinson example, pg. 2-4

text <- c("Because I could not stop for Death -",
          "He kindly stopped for me -",
          "The Carriage held but just Ourselves -",
          "and Immortality")

text_df <- tibble(line = 1:4, text = text)
text_df

text_df %>% 
  unnest_tokens(word, text)

text_df %>% 
  unnest_tokens(word, text, to_lower = FALSE)

```

```{r, echo = T, eval = F}
# 'janeaustenr' intro, pg. 4-7

library(janeaustenr)

original_books <- austen_books() %>% 
  group_by(book) %>% 
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]",
                                                 ignore_case = TRUE)))) %>% 
  ungroup()
original_books

tidy_books <- original_books %>% 
  unnest_tokens(word, text)
tidy_books

data(stop_words)
tidy_books <- tidy_books %>% 
  anti_join(stop_words, by = "word")

tidy_books %>% 
  count(word, sort = TRUE)

tidy_books %>% 
  count(word, sort = TRUE) %>% 
  filter(n > 600) %>% 
  mutate(word = reorder(word, n)) %>% 
  ggplot(aes(word, n)) + geom_col() + xlab(NULL) + coord_flip()

```

**stop words**: *read more [here](https://en.wikipedia.org/wiki/Stop_words)*

```{r, echo = T, eval = F, message = F, warning = F}
# 'gutenbergr' intro, pg. 7-12

library(gutenbergr)

## H.G. Wells

hgwells <- gutenberg_download(c(35, 36, 5230, 159))

tidy_hgwells <- hgwells %>% 
  unnest_tokens(word, text) %>% 
  anti_join(stop_words, by = "word")

tidy_hgwells %>% 
  count(word, sort = TRUE)

## Brontë Sisters

bronte <- gutenberg_download(c(1260, 768, 969, 9182, 767))

tidy_bronte <- bronte %>% 
  unnest_tokens(word, text) %>% 
  anti_join(stop_words, by = "word")

tidy_bronte %>% 
  count(word, sort = TRUE)

## binding both with Austen...

frequency <- bind_rows(mutate(tidy_bronte, author = "Brontë Sisters"),
                       mutate(tidy_hgwells, author = "H.G. Wells"),
                       mutate(tidy_books, author = "Jane Austen")) %>% 
  mutate(word = str_extract(word, "[a-z']+")) %>% 
  count(author, word) %>% 
  group_by(author) %>% 
  mutate(proportion = n / sum(n)) %>% 
  select(-n) %>% 
  spread(author, proportion) %>% 
  gather(author, proportion, `Brontë Sisters`:`H.G. Wells`)

ggplot(frequency, aes(x = proportion, y = `Jane Austen`,
                      color = abs(`Jane Austen` - proportion))) +
  geom_abline(color = "gray40", lty = 2) + 
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) + 
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) + 
  scale_x_log10(labels = percent_format()) + 
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.001),
                       low = "darkslategray4", high = "gray75") +
  facet_wrap(~author, ncol = 2) +
  theme(legend.position = "none") +
  labs(y = "Jane Austen", x = NULL)
  
## Correlation test

cor.test(data = frequency[frequency$author == "Brontë Sisters",],
         ~ proportion + `Jane Austen`)

cor.test(data = frequency[frequency$author == "H.G. Wells",],
         ~ proportion + `Jane Austen`)

```

***Chapter 1 summary***: 

Discusses tokens and tokenization, as well as presents some interesting ways to visualize relationships between certain literary artists. 

Code takeaways include `unnest_tokens()` (from package `tidytext`), basic regular expressions as used in `regex()`, packages `janeaustenr` and `gutenbergr` for playing around, and the package `scales` which includes the `"free_y"` option in `facet_wrap()`. 

Refer back to later code chunks to see an intuitive way to view correlation plots and visualize word frequency. The "tidy text" format refers to text broken down into tokens, labeled by line, chapter, etc. 

***

## Chapter 2: Sentiment Analysis with Tidy Data

***

```{r, eval = F}
# Sentiment analysis intro, pg. 13-19

sentiments  ## included in package 'tidytext'...

get_sentiments("afinn")   ## ...divided into three lexicons
get_sentiments("bing")
get_sentiments("nrc")

## Testing sentiments with Austen's 'Emma'

tidy_books <- austen_books() %>% 
  group_by(book) %>% 
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]",
                                                 ignore_case = TRUE)))) %>% 
  ungroup() %>% 
  unnest_tokens(word, text)

nrcjoy <- get_sentiments("nrc") %>% 
  filter(sentiment == "joy")

tidy_books %>% 
  filter(book == "Emma") %>% 
  inner_join(nrcjoy, by = "word") %>% 
  count(word, sort = TRUE)

## Comparing sentiment across Austen's works

janeaustensentiment <- tidy_books %>% 
  inner_join(get_sentiments("bing"), by = "word") %>% 
  count(book, index = linenumber %/% 80, sentiment) %>% 
  spread(sentiment, n, fill = 0) %>% 
  mutate(sentiment = positive - negative)
  
ggplot(janeaustensentiment, aes(index, sentiment, fill = book)) + 
  geom_col(show.legend = FALSE) +
  facet_wrap(~ book, ncol = 2, scales = "free_x")
  
```

```{r, eval = F}
# Examining sentiment in Austen's 'Pride & Prejudice', pg. 19-22

pride_prejudice <- tidy_books %>% 
  filter(book == "Pride & Prejudice")

afinn <- pride_prejudice %>% 
  inner_join(get_sentiments("afinn"), by = "word") %>% 
  group_by(index = linenumber %/% 80) %>% 
  summarize(sentiment = sum(value)) %>%       ## book has "value" as "score"
  mutate(method = "AFINN")

bing_and_nrc <- bind_rows(
  pride_prejudice %>% 
    inner_join(get_sentiments("bing"), by = "word") %>% 
    mutate(method = "Bing et al."),
  pride_prejudice %>% 
    inner_join(get_sentiments("nrc"), by = "word") %>% 
                 filter(sentiment %in% c("positive", "negative")) %>% 
    mutate(method = "NRC")) %>% 
  count(method, index = linenumber %/% 80, sentiment) %>% 
  spread(sentiment, n, fill = 0) %>% 
  mutate(sentiment = positive - negative)

bind_rows(afinn, bing_and_nrc) %>% 
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ method, ncol = 1, scales = "free_y")

## Why is NRC biased so high?

get_sentiments("nrc") %>% 
  filter(sentiment %in% c("positive", "negative")) %>% 
  count(sentiment)

get_sentiments("bing") %>% 
  count(sentiment)

get_sentiments("afinn") %>% 
  count(value)

```

```{r, eval = F, message = F}
# Common positive and negative words, pg. 22-24

bing_word_counts <- tidy_books %>% 
  inner_join(get_sentiments("bing"), by = "word") %>% 
  count(word, sentiment, sort = TRUE) %>% 
  ungroup()

bing_word_counts

bing_word_counts %>% 
  group_by(sentiment) %>% 
  top_n(10) %>% 
  ungroup() %>% 
  mutate(word = reorder(word, n)) %>% 
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",
       x = NULL) +
  coord_flip()

## The word "miss" is a title, not negative...

custom_stop_words <- bind_rows(tibble(word = c("miss"),
                                      lexicon = c("custom")), 
                               stop_words)

custom_stop_words

```

```{r, eval = F, message = F, warning = F}
# Wordcloud fun, pg. 25-26

library(wordcloud)

tidy_books %>% 
  anti_join(stop_words) %>% 
  count(word) %>% 
  with(wordcloud(word, n, max.words = 100))

## Turning data frame into matrix for comparison.cloud()

library(reshape2)

tidy_books %>% 
  inner_join(get_sentiments("bing"), by = "word") %>% 
  count(word, sentiment, sort = TRUE) %>% 
  acast(word ~ sentiment, value.var = "n", fill = 0) %>% 
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)

```

```{r, eval = F, message = F}
# Units / tokenizing beyond words..., pg. 27-29

PandP_sentences <- tibble(text = prideprejudice) %>% 
  unnest_tokens(sentence, text, token = "sentences")

PandP_sentences$sentence[2]   ## not ideal... brief ways to fix on pg. 27

## Try using a regex() pattern to divide and analyze chapters as tokens

austen_chapters <- austen_books() %>% 
  group_by(book) %>% 
  unnest_tokens(chapter, text, token = "regex", 
                pattern = "Chapter|CHAPTER [\\dIVXLC]") %>% 
  ungroup()

austen_chapters %>% 
  group_by(book) %>% 
  summarize(chapters = n())

bingnegative <- get_sentiments("bing") %>% 
  filter(sentiment == "negative")

wordcounts <- tidy_books %>% 
  group_by(book, chapter) %>% 
  summarize(words = n())

tidy_books %>% 
  semi_join(bingnegative) %>% 
  group_by(book, chapter) %>% 
  summarize(negativewords = n()) %>% 
  left_join(wordcounts, by = c("book", "chapter")) %>% 
  mutate(ration = negativewords/words) %>% 
  filter(chapter != 0) %>% 
  top_n(1) %>% 
  ungroup()

```

***Chapter 2 summary***: 

Discusses sentiment analysis and investigates options for conducting this as well as interesting routes of visualization.

Code takeaways include the `sentiments` dataset in package `tidytext` with `get_sentiments()` (with options `"bing"`, `"afinn"`, and `"nrc"`), `wordcloud()` and `comparison.cloud()` (from package `wordcloud`), `acast()` (from package `reshape2`), advanced packages `coreNLP` / `cleanNLP` / `sentimentr` for sentiment analysis beyond words, and the `token =` option in `unnest_tokens()`

Refer back to this section for help tidying data for sentiment analysis and using various graphics to do so.


***

## Chapter 3: Analyzing Word and Document Frequency: tf-idf

***

```{r, eval = F}
# Sentiment analysis intro, pg. 13-19


## Testing sentiments with Austen's 'Emma'


## Comparing sentiment across Austen's works


  
```


